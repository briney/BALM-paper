{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ac145b-5b4a-4a18-82cb-2ee4d5b1f1ce",
   "metadata": {},
   "source": [
    "# extracting cross-chain attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e097b6-c89f-4b5e-9422-85642a26badf",
   "metadata": {},
   "source": [
    "This code generates the attention matrix for 5 therapeutic monoclonal antibodies (found in the therapetuic-mAbs.csv file) and exports the results as a csv file for each antibody.\n",
    "\n",
    "The resulting csv files can be read into the Plot-Cross-Chain-Attention.ipynb file to plot the attention values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406b4a0-8920-4892-9a12-cd78714dc27b",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee945e-d23f-4e17-8f94-d6080d2831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8912caf-410a-49ef-b554-1869a4032b23",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efc970-5eda-4a88-b310-7413b68a4814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download the fine-tuned ESM-2 model from zenodo\n",
    "if [ ! -d \"./models/ESM2-650M_paired-fine-tuning\" ]; then\n",
    "    curl -o 'ESM2-650M_paired-fine-tuned.tar.gz' -L 'https://zenodo.org/record/8253367/files/ESM2-650M_paired-fine-tuned.tar.gz?download=1'\n",
    "    tar xzvf 'ESM2-650M_paired-fine-tuned.tar.gz' -C ./models\n",
    "    rm 'ESM2-650M_paired-fine-tuned.tar.gz'\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd7b41-954e-4e2d-b0fc-8168786d67df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    './models/ESM2-650M_paired-fine-tuning/'\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487cbb8-3a1d-4730-8e84-161edbb48966",
   "metadata": {},
   "source": [
    "If you want to load the 650M parameter ESM-2 model prior to fine-tuning instead, uncomment the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bee96-0f9e-4ff3-95d4-09ccb1adfb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForMaskedLM.from_pretrained(\n",
    "#    \"facebook/esm2_t33_650M_UR50D\"\n",
    "# ).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ed17f-13cb-4c9e-b6bd-bd00b6624251",
   "metadata": {},
   "source": [
    "### load + tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae573ea0-298e-4bb0-97c6-c60b549db332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load therapeutic antibody sequences\n",
    "df = pd.read_csv('./therapeutic-mAbs.csv')\n",
    "seq_df = df[[\"Therapeutic\", \"Heavy Sequence\", \"Light Sequence\"]].set_index(\"Therapeutic\")\n",
    "\n",
    "# Concat heavy and light chain sequences\n",
    "seqs = []\n",
    "for h, l in zip(seq_df['Heavy Sequence'], seq_df['Light Sequence']):\n",
    "    seqs.append(\"{}<cls><cls>{}\".format(h, l))\n",
    "seq_names = list(seq_df.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15cbe6-685e-4529-95e0-a6316540e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44b33f-85f6-4db1-9bbf-45e4c6f046c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for s in tqdm(seqs):\n",
    "    tokenized_data.append(tokenizer(s, return_tensors='pt').to('cuda'))\n",
    "    \n",
    "i = {'input_ids': [t['input_ids'] for t in tokenized_data],\n",
    "     'attention_mask': [t['attention_mask'] for t in tokenized_data]}\n",
    "\n",
    "# finalize inputs\n",
    "inputs = list(zip(seq_names, seqs, tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc585f36-4cf3-4ee0-8487-95ee26007a37",
   "metadata": {},
   "source": [
    "## generate and export attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721018-f05b-4a6d-946a-1a258fd27c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    for name, seq, tokens in tqdm(inputs):\n",
    "        print(f\"Input: {name}\")\n",
    "        \n",
    "        outputs = model(\n",
    "            **tokens, \n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        # parse the sequence\n",
    "        h, l = seq.split('<cls><cls>')\n",
    "        h_positions = list(range(1, len(h) + 1))\n",
    "        l_positions = list(range(len(h) + 2, len(h) + 2 + len(l)))\n",
    "        all_positions = h_positions + l_positions\n",
    "        \n",
    "        # Get the attention values for each layer and attention head\n",
    "        attentions = outputs.attentions\n",
    "        num_layers = len(attentions)\n",
    "        num_heads = attentions[0].size(1)\n",
    "\n",
    "        # Extract attention values for each attention head in every layer\n",
    "        all_attentions = []\n",
    "        for layer in tqdm(range(num_layers)[:]): #for each layer\n",
    "            layer_attentions = attentions[layer]\n",
    "            \n",
    "            for head in range(num_heads): #for each head in that layer\n",
    "                head_attentions = layer_attentions[0, head]\n",
    "                for p1 in all_positions:\n",
    "                    for p2 in all_positions:\n",
    "                        p1_region = \"heavy\" if p1 in h_positions else \"light\"\n",
    "                        p2_region = \"heavy\" if p2 in h_positions else \"light\"\n",
    "                        comp_type = f\"intra-{p1_region}\" if p1_region == p2_region else \"cross-chain\"\n",
    "                        all_attentions.append(\n",
    "                            {\n",
    "                                \"position1\": p1,\n",
    "                                \"position2\": p2,\n",
    "                                \"comparison\": comp_type,\n",
    "                                \"attention\": head_attentions[p1, p2].item(),\n",
    "                                \"layer\": layer,\n",
    "                                \"head\": head\n",
    "                            }\n",
    "                        )\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        attention_df = pd.DataFrame(all_attentions)\n",
    "        \n",
    "        # Export to csv\n",
    "        attention_df.to_csv(f'./attention-results/{name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
