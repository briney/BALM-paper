{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfde80b-6ab0-4fa0-a88d-dd31fb2841e0",
   "metadata": {},
   "source": [
    "### HD vs CoV classification task\n",
    "\n",
    "To adapt this code to the **Flu vs CoV** classification task, you'll need to change the dataset, the class labels, and the batch size (to 8 total). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359d750-75e6-46db-a2ba-16701e01c135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    ClassLabel,\n",
    "    load_dataset,\n",
    ")\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "from datetime import date\n",
    "from random import randint\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c83e4-5d28-4b6e-9329-40244a6cc17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace with actual model path\n",
    "checkpoint = './BALM-paired/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab7336-79aa-4963-a854-5aa7b8dcef97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"../tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b6391-f608-4bc5-9b76-23da04f12c47",
   "metadata": {},
   "source": [
    "### Process and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce4ae0-fefd-485d-a383-f947463fd12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = ClassLabel(names=['Healthy-donor','Sars-specific'])\n",
    "n_classes = len(class_labels.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfacd5-e1fc-485c-84e6-ddc0bf4103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset provided in zenodo is the full dataset (not split into train-test)\n",
    "# so you'll need to do your dataset split(s) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90412989-98e6-4607-ac96-d2f7d742e65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itr_datasets = []\n",
    "for i in range(5):\n",
    "    data_files = DatasetDict({\n",
    "        'train': f'./datasets/HD-CoV/hd-0_cov-1_train{i}.csv',\n",
    "        'test': f'./datasets/HD-CoV/hd-0_cov-1_test{i}.csv'\n",
    "    })\n",
    "    dataset = load_dataset('csv', data_files=data_files)\n",
    "    itr_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee5838-0f79-4465-ad1b-b2ca3b0220b6",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a09c5c-67b0-451a-af36-91b032071282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    batch, \n",
    "    tokenizer=None, \n",
    "    tokenizer_path=\"./tokenizer\", \n",
    "    separator=\"</s>\",\n",
    "    max_len=320\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    docstring\n",
    "    \"\"\"\n",
    "    # set up tokenizer if not provided\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, max_len=max_len)\n",
    "        \n",
    "    # tokenize the H/L sequence pair\n",
    "    sequences = [h + separator + l for h, l in zip(batch[\"h_sequence\"], batch[\"l_sequence\"])]\n",
    "    tokenized = tokenizer(sequences, padding=\"max_length\", max_length=max_len, truncation=True)\n",
    "    batch[\"input_ids\"] = tokenized.input_ids\n",
    "    batch[\"attention_mask\"] = tokenized.attention_mask\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14225833-b3ef-43ac-9e84-4bd193560618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for dataset in itr_datasets:\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_dataset,\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"max_len\": 320,\n",
    "        },\n",
    "        batched=True,\n",
    "        remove_columns=[\"name\", \"h_sequence\", \"l_sequence\"]\n",
    "    )\n",
    "    tokenized.append(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2361b6-8fd4-460a-9a0a-dae1c2b18a8f",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98757bc-2571-48fc-9ee9-8c6be1063434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id = {\"Healthy-donor\": 0, \"Sars-specific\": 1}\n",
    "id2label = {0: \"Healthy-donor\", 1: \"Sars-specific\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc759e29-2537-473e-902f-14afa2b883b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fig 5 presents accuracy, f1, auc, aupr, and mcc\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    probabilities = torch.softmax(torch.from_numpy(predictions), dim=1).detach().numpy()[:,-1]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    _accuracy = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\n",
    "        \"accuracy\": _accuracy,\n",
    "        \"precision\": precision_score(labels, predictions, pos_label=1),\n",
    "        \"recall\": recall_score(labels, predictions, pos_label=1),\n",
    "        \"f1\": f1_score(labels, predictions, pos_label=1),\n",
    "        \"auc\": roc_auc_score(labels, probabilities),\n",
    "        \"aupr\": average_precision_score(labels, probabilities, pos_label=1),\n",
    "        \"mcc\": matthews_corrcoef(labels, predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfda37-9de5-4b6b-89b2-c03ada41e0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\"itr\": [],\n",
    "                             \"test_loss\": [],\n",
    "                             \"test_accuracy\": [],\n",
    "                             \"test_precision\": [],\n",
    "                             \"test_recall\": [],\n",
    "                             \"test_f1\": [],\n",
    "                             \"test_auc\": [],\n",
    "                             \"test_aupr\": [],\n",
    "                             \"test_mcc\": [],\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0d9c0-a0d5-48ad-a69c-22a979b16c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n, dataset in enumerate(tokenized):\n",
    "    itr = n\n",
    "    run_name = f\"BALM-paired_HD-CoV_itr-{itr}_{date.today().isoformat()}\"\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, \n",
    "        num_labels=n_classes,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    # use this to freeze the base model weights + train only classification head\n",
    "    # for param in model.base_model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    batch_size = 32 # on 1 gpu (ie. total batch size should equal 32)\n",
    "    lr = 5e-5\n",
    "    training_args = TrainingArguments(\n",
    "        evaluation_strategy = \"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        eval_steps=10,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size, \n",
    "        per_device_eval_batch_size=batch_size, \n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='linear',\n",
    "\n",
    "        output_dir=f\"./checkpoints/{run_name}\",\n",
    "        seed=randint(0, 1024),\n",
    "        report_to=\"wandb\",\n",
    "        logging_dir=f\"./logs/{run_name}\",\n",
    "        logging_first_step=True,\n",
    "        run_name = run_name\n",
    "    )\n",
    "    \n",
    "    wandb.init(\n",
    "        project = 'specificity-class',\n",
    "        group=\"HD-CoV\",\n",
    "        job_type=\"BALM-paired\",\n",
    "        name = run_name,\n",
    "        dir = './',\n",
    "    )\n",
    "    \n",
    "    # train\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"./models/{run_name}\")\n",
    "    wandb.finish()\n",
    "    \n",
    "    # evaluate\n",
    "    logits, labels, metrics = trainer.predict(dataset['test'])\n",
    "    metrics['itr'] = itr\n",
    "    test_results = test_results.append(metrics, ignore_index=True)\n",
    "    \n",
    "    del model # delete to ensure untrained model is being trained for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2af196-8cfa-4d32-b628-d3a648997728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results.loc['mean'] = test_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f87d6-b586-479d-9207-7450c2db324c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results.loc['std'] = test_results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e36c22-88b0-46e4-b193-2410c366d216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results.to_csv(f'./results/HD-CoV_BALM-paired.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
