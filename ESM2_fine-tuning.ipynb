{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6db0d1-0f5c-4743-b711-fc0613f37dc3",
   "metadata": {},
   "source": [
    "# Fine-tuning ESM-2 with natively paired Ab sequences\n",
    "\n",
    "#### architecture\n",
    "[ESM-2](https://www.science.org/doi/10.1126/science.ade2574) is a state-of-the-art, general purpose protein LM that uses a modified BERT architecture. Aside from model size, the primary modification is the use of rotary position encoding ([RoPE](https://arxiv.org/abs/2104.09864)) rather than absolute position embedding. Due to compute constraints, we performed a full fine-tuning of the 650M-parameter variant of ESM-2.\n",
    "\n",
    "#### dataset\n",
    "We used all unique, productive paired sequences reported in [_Functional antibodies exhibit light chain coherence_](https://www.nature.com/articles/s41586-022-05371-z) (Jaffe et al, Nature 2022)\n",
    "* [dataset DOI](https://plus.figshare.com/articles/dataset/Dataset_supporting_Functional_antibodies_exhibit_light_chain_coherence_/20338177) (figshare)\n",
    "* the dataset was split in to train/eval/test subsets at a ratio of 90:5:5, which produced the following dataset sizes:\n",
    "    * **train**: `1,202,269` paired sequences\n",
    "    * **eval**: `66,791` paired sequences\n",
    "    * **test**: `66,971` paired sequences  \n",
    "    \n",
    "Each input file contains a single paired antibody amino acid sequence per line, with the heavy and light chain sequences concatenated and separated by two `<cls>` tokens, like so:  \n",
    "  > `HEAVY_CHAIN_AA_SEQUENCE<cls><cls>LIGHT_CHAIN_AA_SEQUENCE`\n",
    "\n",
    "#### training\n",
    "Full fine-tuning ESM-2 on eight NVIDIA A100 GPUs took approximately 10 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b975c-2916-4612-a55c-a795d687f063",
   "metadata": {},
   "source": [
    "<br>  \n",
    "  \n",
    "## setup  \n",
    "\n",
    "Fine-tuning ESM-2 requires several [huggingface](https://huggingface.co/) libraries. If they're not already installed, you can install them by uncommenting and running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a24cf3-862a-4a88-89a6-04a06a9d5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ed1d2",
   "metadata": {},
   "source": [
    "## ESM-2 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"ESM-2_fine-tuning_{date.today().isoformat()}\"\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "esm_config = {\n",
    "    \"run_name\": run_name,\n",
    "    \n",
    "    # training parameters\n",
    "    \"batch_size\": 32,\n",
    "    \"max_steps\": 150000,\n",
    "    \"warmup_steps\": 30000,\n",
    "    \"save_steps\": 50000,\n",
    "    \"logging_steps\": 100,\n",
    "    \"eval_steps\": 25000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"peak_learning_rate\": 4e-4,\n",
    "    \"adam_epsilon\": 1e-6,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.98,\n",
    "    \n",
    "    # outputs and logging\n",
    "    \"output_dir\": f\"./checkpoints/{run_name}\",  # where the checkpoint data will be written\n",
    "    \"report_to\": \"wandb\",  # enable logging to w&b\n",
    "    \"logging_dir\": f\"./logs/{run_name}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d774a2-11f0-4984-93af-e5d61ecfd112",
   "metadata": {},
   "source": [
    "<br>  \n",
    "  \n",
    "If you'd like to use [weights and biases](https://wandb.ai) for logging, uncomment and run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745fed8-7a0a-436b-b3a2-f4a851218161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_PROJECT\"] = run_name\n",
    "# balm_config[\"report_to\"] = \"wandb\"\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f974c7-20f1-4cf8-be37-162f17006290",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59238ba7-e766-46bd-8477-5596cf4842cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c70e17-0cd7-469f-a571-a1eac2dfe309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabdd9a-90d8-4916-87f6-faa5d5eca798",
   "metadata": {},
   "source": [
    "## load + tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25a946-8ad4-4965-abe9-98763ae3d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download the train/eval/test data if it doesn't exist\n",
    "if [ ! -d \"./data/train-test-eval_paired\" ]; then\n",
    "    curl -o 'train-test-eval_paired.tar.gz' -L 'https://zenodo.org/record/8253367/files/train-test-eval_paired.tar.gz?download=1'\n",
    "    tar xzvf 'train-test-eval_paired.tar.gz' -C ./data\n",
    "    rm 'train-test-eval_paired.tar.gz'\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653cc7f6-4b59-40fd-acc0-6fdb128396b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": ['./data/train-test-eval_paired/train.txt'],\n",
    "    \"eval\": ['./data/train-test-eval_paired/eval.txt'],\n",
    "    \"test\": ['./data/train-test-eval_paired/test.txt']\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files)\n",
    "\n",
    "# reformat dataset so that HC and LC are seperated by <cls><cls> instead of </s>\n",
    "dataset = dataset.map(lambda x: {\"text\": x[\"text\"].replace(\"</s>\", \"<cls><cls>\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dec8ea-c338-4c95-8226-98c8e5402205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "        return_special_tokens_mask=True,\n",
    "    ),\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861a642-b5e2-4270-8bd5-75b9f3375c19",
   "metadata": {},
   "source": [
    "## data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5d205-2e54-4952-9a16-d684cca11cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd0b42-847c-4ce0-94c7-0c44a3e37a1a",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1238a76-ee62-41af-8cda-123f6d2f629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=esm_config.get(\"batch_size\", 32),\n",
    "    per_device_eval_batch_size=esm_config.get(\"batch_size\", 32),\n",
    "    max_steps=esm_config.get(\"max_steps\", 500000),\n",
    "    save_steps=esm_config.get(\"save_steps\", 50000),\n",
    "    logging_steps=esm_config.get(\"logging_steps\", 100),\n",
    "    eval_steps=esm_config.get(\"eval_steps\", 25000),\n",
    "    adam_beta1=esm_config.get(\"adam_beta1\", 0.9),\n",
    "    adam_beta2=esm_config.get(\"adam_beta2\", 0.98),\n",
    "    adam_epsilon=esm_config.get(\"adam_epsilon\", 1e-6),\n",
    "    weight_decay=esm_config.get(\"weight_decay\", 0.01),\n",
    "    warmup_steps=esm_config.get(\"warmup_steps\", 30000),\n",
    "    learning_rate=esm_config.get(\"peak_learning_rate\", 4e-4),\n",
    "    gradient_accumulation_steps=esm_config.get(\"gradient_accumulation_steps\", 1),\n",
    "    \n",
    "    # output and logging\n",
    "    output_dir=esm_config.get(\"output_dir\", f\"./checkpoints/{run_name}\"),\n",
    "    overwrite_output_dir=True,\n",
    "    logging_dir=esm_config.get(\"logging_dir\", f\"./logs/{run_name}\"),\n",
    "    report_to=esm_config.get(\"report_to\", None),\n",
    "    run_name=run_name,  # name of the W&B run\n",
    "    logging_first_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb1783-a44e-4ca5-8b8b-b7b6b52349d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6677b-732f-4cfb-958b-590d2510cf63",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df2bca-8349-4ce6-91f3-15c7f7c0999f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846bcae-f847-4e93-acc1-500554041275",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"./models/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937a75e-2608-402b-a26d-3e28dd566ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
