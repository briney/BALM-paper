{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ac145b-5b4a-4a18-82cb-2ee4d5b1f1ce",
   "metadata": {},
   "source": [
    "# final layer cross-chain attention - average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e097b6-c89f-4b5e-9422-85642a26badf",
   "metadata": {},
   "source": [
    "This code extracts the final layer cross-chain attention of BALM-paired for 1000 antibodies from our test dataset and exports the results as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406b4a0-8920-4892-9a12-cd78714dc27b",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee945e-d23f-4e17-8f94-d6080d2831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForMaskedLM\n",
    ")\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8912caf-410a-49ef-b554-1869a4032b23",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5b1a4-3716-4f39-af34-52858b6fc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with actual model path\n",
    "model_path = './BALM-paired/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bee96-0f9e-4ff3-95d4-09ccb1adfb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ed17f-13cb-4c9e-b6bd-bd00b6624251",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load + tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878694c-9229-4080-8bd4-acd23fe21d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with actual data path\n",
    "data_path = './test_dataset_1000seqs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eaedc3-a5ea-4665-9487-0a12a40221fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected = pd.read_csv(data_path)\n",
    "df_selected['text'] = df_selected['text'].str.replace('<cls><cls>', '</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae573ea0-298e-4bb0-97c6-c60b549db332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = list(df_selected['text'])\n",
    "seq_names = list(df_selected['sequence_id'])\n",
    "cdrs = list(df_selected['cdr_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15cbe6-685e-4529-95e0-a6316540e7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"../tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44b33f-85f6-4db1-9bbf-45e4c6f046c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for s in tqdm(seqs):\n",
    "    tokenized_data.append(tokenizer(s, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198f8bc-89f5-4dcb-b09a-9d2553deae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize inputs\n",
    "inputs = list(zip(seq_names, seqs, tokenized_data, cdrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc585f36-4cf3-4ee0-8487-95ee26007a37",
   "metadata": {},
   "source": [
    "## functions for processing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d043dc3-c4fe-4c6d-9711-1f571e288d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_heads(cc_attention_df):\n",
    "    head_dfs = []\n",
    "    for head in sorted(cc_attention_df[\"head\"].unique()):\n",
    "        cc_attention_sq = pd.pivot(\n",
    "            data = cc_attention_df[cc_attention_df[\"head\"] == head], \n",
    "            index=\"position1\", \n",
    "            columns=\"position2\", \n",
    "            values=\"attention\",\n",
    "        )\n",
    "\n",
    "        hlen = sum(cc_attention_sq[1].isna())\n",
    "        hl_sqdf = cc_attention_sq.iloc[:hlen, hlen:]\n",
    "        lh_sqdf = cc_attention_sq.iloc[hlen:, :hlen].T\n",
    "\n",
    "        light = hl_sqdf.mean(axis=0) # attention to the light chain (from the heavy)\n",
    "        heavy = lh_sqdf.mean(axis=1) # attention to the heavy chain (from the light) \n",
    "\n",
    "        sum_sqdf = pd.concat([heavy, light])\n",
    "\n",
    "        head_dfs.append(sum_sqdf)\n",
    "        \n",
    "    head_dfs = sum(head_dfs) / 16\n",
    "    \n",
    "    return head_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0225b-e1e1-4547-9a40-f7798736310f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_attention(seq_id, cdrs, attention_by_pos):\n",
    "    count_non_cdr = 0.0\n",
    "    count_cdr = 0.0\n",
    "    num_cdr_pos = 0\n",
    "    total_pos = 0\n",
    "    for cdr, num in zip(cdrs, attention_by_pos):\n",
    "        total_pos += 1\n",
    "        if float(cdr) == 1:\n",
    "            count_cdr += num\n",
    "            num_cdr_pos += 1\n",
    "        else:\n",
    "            count_non_cdr += num\n",
    "            \n",
    "    total_atten = count_cdr + count_non_cdr\n",
    "    cdr_perc = count_cdr / total_atten * 100\n",
    "    non_cdr_perc = count_non_cdr / total_atten * 100\n",
    "    \n",
    "    cdr_seq_perc = num_cdr_pos / total_pos * 100\n",
    "    \n",
    "    data = [[seq_id, total_pos, num_cdr_pos, count_cdr, count_non_cdr, cdr_seq_perc, cdr_perc, non_cdr_perc]]\n",
    "    df = pd.DataFrame(data, columns=['Sequence_id',\n",
    "                                     'Num-Total-Pos', \n",
    "                                     'Num-CDR-Pos',\n",
    "                                     'CDR-atten', \n",
    "                                     'Non-CDR-atten',\n",
    "                                     'CDR-seq%',\n",
    "                                     'CDR-atten%',\n",
    "                                     'Non-CDR-atten%'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e1a1f-4100-45dc-9e76-959939cc471b",
   "metadata": {},
   "source": [
    "## generate and export attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156dd79-3b83-451f-8142-82637f4e9120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Sequence_id',\n",
    "                                'Num-Total-Pos', \n",
    "                                'Num-CDR-Pos',\n",
    "                                'CDR-atten', \n",
    "                                'Non-CDR-atten',\n",
    "                                'CDR-seq%',\n",
    "                                'CDR-atten%',\n",
    "                                'Non-CDR-atten%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721018-f05b-4a6d-946a-1a258fd27c2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    for name, seq, tokens, cdrs in tqdm(inputs):\n",
    "        print(f\"Input: {name}\")\n",
    "        \n",
    "        outputs = model(\n",
    "            **tokens, \n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        # parse the sequence\n",
    "        h, l = seq.split('</s>')\n",
    "        h_positions = list(range(1, len(h) + 1))\n",
    "        l_positions = list(range(len(h) + 2, len(h) + 2 + len(l)))\n",
    "        all_positions = h_positions + l_positions\n",
    "        \n",
    "        # Get the attention values for each layer and attention head\n",
    "        attentions = outputs.attentions\n",
    "        num_heads = attentions[0].size(1)\n",
    "\n",
    "        layer = 23\n",
    "        all_attentions = []\n",
    "        layer_attentions = attentions[layer] # for last layer only\n",
    "        for head in tqdm(range(num_heads)): # for each head in that layer\n",
    "            head_attentions = layer_attentions[0, head]\n",
    "            for p1 in all_positions:\n",
    "                for p2 in all_positions:\n",
    "                    p1_region = \"heavy\" if p1 in h_positions else \"light\"\n",
    "                    p2_region = \"heavy\" if p2 in h_positions else \"light\"\n",
    "                    comp_type = f\"intra-{p1_region}\" if p1_region == p2_region else \"cross-chain\"\n",
    "                    all_attentions.append(\n",
    "                        {\n",
    "                            \"position1\": p1,\n",
    "                            \"position2\": p2,\n",
    "                            \"comparison\": comp_type,\n",
    "                            \"attention\": head_attentions[p1, p2].item(),\n",
    "                            \"layer\": layer,\n",
    "                            \"head\": head\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        attention_df = pd.DataFrame(all_attentions)\n",
    "        \n",
    "        \n",
    "        # Cross-chain attention\n",
    "        cdrs = cdrs.replace(\"--\",\"\")\n",
    "        atten_cc = attention_df[attention_df[\"comparison\"] == \"cross-chain\"]\n",
    "        layer_avg = avg_heads(atten_cc)\n",
    "        cc_df = calculate_attention(name, cdrs, layer_avg)\n",
    "\n",
    "        results = pd.concat([results, cc_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf1f24-9b66-4f53-9a56-c7e4c52c095d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa430be-35ab-4100-8849-5881c418e5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa19883-fae2-43d9-a64d-0cc428a82783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv('./attention-results/BALM-paired_1kattention-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
