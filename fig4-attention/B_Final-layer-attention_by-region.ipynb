{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ac145b-5b4a-4a18-82cb-2ee4d5b1f1ce",
   "metadata": {},
   "source": [
    "# final layer cross-chain attention - by region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e097b6-c89f-4b5e-9422-85642a26badf",
   "metadata": {},
   "source": [
    "This code extracts the final layer cross-chain attention of ft-ESM for 1000 antibodies (and averages by FR and CDR regions) from our test dataset and exports the results as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406b4a0-8920-4892-9a12-cd78714dc27b",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee945e-d23f-4e17-8f94-d6080d2831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    EsmForMaskedLM\n",
    ")\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8912caf-410a-49ef-b554-1869a4032b23",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51b3a9-a01c-4db9-842c-cffb0efea401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with actual model path\n",
    "model_path = './ft-ESM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bee96-0f9e-4ff3-95d4-09ccb1adfb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = EsmForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ed17f-13cb-4c9e-b6bd-bd00b6624251",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load + tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc458631-7300-49f2-9427-1399a8cf3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with actual data path\n",
    "data_path = './test_dataset_1000seqs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eaedc3-a5ea-4665-9487-0a12a40221fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae573ea0-298e-4bb0-97c6-c60b549db332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = list(df_selected['text'])\n",
    "seq_names = list(df_selected['sequence_id'])\n",
    "cdrs = list(df_selected['cdr_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15cbe6-685e-4529-95e0-a6316540e7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44b33f-85f6-4db1-9bbf-45e4c6f046c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for s in tqdm(seqs):\n",
    "    tokenized_data.append(tokenizer(s, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff25c1-f9f6-457e-8e92-3d84ed716d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(zip(seq_names, seqs, tokenized_data, cdrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc585f36-4cf3-4ee0-8487-95ee26007a37",
   "metadata": {},
   "source": [
    "## functions for processing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccb13e-83c1-42b6-ac71-f83256efb1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average heads for cross-chain attention\n",
    "def avg_heads(cc_attention_df):\n",
    "    head_dfs = []\n",
    "    for head in sorted(cc_attention_df[\"head\"].unique()):\n",
    "        cc_attention_sq = pd.pivot(\n",
    "            data = cc_attention_df[cc_attention_df[\"head\"] == head], \n",
    "            index=\"position1\", \n",
    "            columns=\"position2\", \n",
    "            values=\"attention\",\n",
    "        )\n",
    "\n",
    "        hlen = sum(cc_attention_sq[1].isna())\n",
    "        hl_sqdf = cc_attention_sq.iloc[:hlen, hlen:]\n",
    "        lh_sqdf = cc_attention_sq.iloc[hlen:, :hlen].T\n",
    "\n",
    "        light = hl_sqdf.mean(axis=0)\n",
    "        heavy = lh_sqdf.mean(axis=1)\n",
    "\n",
    "        sum_sqdf = pd.concat([heavy, light])\n",
    "\n",
    "        head_dfs.append(sum_sqdf)\n",
    "        \n",
    "    head_dfs = sum(head_dfs) / 20\n",
    "    \n",
    "    return head_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555a3b0-d538-457e-a0dd-634908bd1f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attention by cdr\n",
    "def atten_by_cdr(seq_id, cdrs, layer_avg):\n",
    "    cdr_list = [int(bit) for bit in cdrs]\n",
    "    total_pos = len(cdr_list)\n",
    "\n",
    "    group_size = 6\n",
    "    current_group_sum = 0\n",
    "    current_group_length = 0\n",
    "    \n",
    "    group_sums = []\n",
    "    group_lengths = []\n",
    "    zero_sums = 0\n",
    "    \n",
    "    for bit, value in zip(cdr_list, layer_avg):\n",
    "        if bit == 1:\n",
    "            current_group_sum += value\n",
    "            current_group_length += 1\n",
    "        elif bit == 0:\n",
    "            zero_sums += value\n",
    "    \n",
    "        if bit == 0 and current_group_sum != 0:\n",
    "            group_sums.append(current_group_sum)\n",
    "            group_lengths.append(current_group_length)\n",
    "            current_group_sum = 0\n",
    "            current_group_length = 0\n",
    "    \n",
    "    if current_group_sum != 0: # check for unfinished group at end\n",
    "        group_sums.append(current_group_sum)\n",
    "        group_lengths.append(current_group_length)\n",
    "\n",
    "    # calculate whole sequence %s\n",
    "    cdr_seq_perc = sum(group_lengths) / total_pos * 100\n",
    "    total_atten = sum(group_sums) + zero_sums\n",
    "    non_cdr_perc = zero_sums / total_atten * 100\n",
    "    cdr_perc = sum(group_sums) / total_atten * 100\n",
    "\n",
    "    # normalize groups for ratio: % of total attention / % of total sequence\n",
    "    avg_non_cdr = (zero_sums * total_pos) / ((total_pos - sum(group_lengths)) * total_atten)\n",
    "    groups_relative = [(i * total_pos)/ (l * total_atten) for i, l in zip(group_sums, group_lengths)]\n",
    "    groups = [\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "    # reformat results\n",
    "    data = [[seq_id, total_pos, sum(group_lengths), sum(group_sums), zero_sums, \n",
    "             cdr_seq_perc, cdr_perc, non_cdr_perc, avg_non_cdr, groups_relative[0], groups_relative[1], \n",
    "             groups_relative[2], groups_relative[3], groups_relative[4], groups_relative[5]]]\n",
    "    df = pd.DataFrame(data, columns=['Sequence_id',\n",
    "                                     'Num-Total-Pos', \n",
    "                                     'Num-CDR-Pos',\n",
    "                                     'CDR-atten', \n",
    "                                     'Non-CDR-atten',\n",
    "                                     'CDR-seq%',\n",
    "                                     'CDR-atten%',\n",
    "                                     'Non-CDR-atten%',\n",
    "                                     'Avg_Non_CDR',\n",
    "                                     \"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f631b6-3548-461e-8904-fec6bad5a139",
   "metadata": {},
   "source": [
    "## generate and export attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156dd79-3b83-451f-8142-82637f4e9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Sequence_id',\n",
    "                                'Num-Total-Pos', \n",
    "                                'Num-CDR-Pos',\n",
    "                                'CDR-atten', \n",
    "                                'Non-CDR-atten',\n",
    "                                'CDR-seq%',\n",
    "                                'CDR-atten%',\n",
    "                                'Non-CDR-atten%',\n",
    "                                'Avg_Non_CDR',\n",
    "                                \"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721018-f05b-4a6d-946a-1a258fd27c2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    for name, seq, tokens, cdrs in tqdm(inputs):\n",
    "        \n",
    "        outputs = model(\n",
    "            **tokens, \n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        # parse the sequence\n",
    "        h, l = seq.split('<cls><cls>')\n",
    "        h_positions = list(range(1, len(h) + 1))\n",
    "        l_positions = list(range(len(h) + 3, len(h) + 3 + len(l)))\n",
    "        all_positions = h_positions + l_positions\n",
    "        \n",
    "        # Get the attention values for each layer and attention head\n",
    "        attentions = outputs.attentions\n",
    "        num_heads = attentions[0].size(1)\n",
    "\n",
    "        layer = 32\n",
    "        all_attentions = []\n",
    "        layer_attentions = attentions[layer] # for last layer only\n",
    "        for head in range(num_heads): # for each head in that layer\n",
    "            head_attentions = layer_attentions[0, head]\n",
    "            for p1 in all_positions:\n",
    "                for p2 in all_positions:\n",
    "                    p1_region = \"heavy\" if p1 in h_positions else \"light\"\n",
    "                    p2_region = \"heavy\" if p2 in h_positions else \"light\"\n",
    "                    comp_type = f\"intra-{p1_region}\" if p1_region == p2_region else \"cross-chain\"\n",
    "                    all_attentions.append(\n",
    "                        {\n",
    "                            \"position1\": p1,\n",
    "                            \"position2\": p2,\n",
    "                            \"comparison\": comp_type,\n",
    "                            \"attention\": head_attentions[p1, p2].item(),\n",
    "                            \"layer\": layer,\n",
    "                            \"head\": head\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        attention_df = pd.DataFrame(all_attentions)\n",
    "        \n",
    "        # Cross-chain attention by cdr group\n",
    "        cdrs = cdrs.replace(\"--\",\"\")\n",
    "        atten_cc = attention_df[attention_df[\"comparison\"] == \"cross-chain\"]\n",
    "        layer_avg = avg_heads(atten_cc)\n",
    "        cc_df = atten_by_cdr(name, cdrs, layer_avg)\n",
    "\n",
    "        results = pd.concat([results, cc_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45782acb-0ff5-459e-96c1-f5f0afa6bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./attention-results/ft-ESM_1kattention-byregion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf324f-462e-4e18-9617-f25d50203b16",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec36a-1cbc-4598-86dd-c6ef70ee5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results.rename(columns = {'Avg_Non_CDR':'FR'})\n",
    "res = res[['FR', 'H1', 'H2', 'H3', 'L1', 'L2', 'L3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9f59a-71b8-40b4-8e77-278861a3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = res.mean(axis=0)\n",
    "std = res.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18761c-8520-4a5d-b849-9c11e54d8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#833f94', '#259c8d', '#259c8d', '#259c8d', '#259c8d', '#259c8d', '#259c8d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1182c13-c2c7-4bb4-910d-3e04d7d88b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4, 3])\n",
    "plt.bar(\n",
    "    mean.index, \n",
    "    mean, \n",
    "    #yerr = std,\n",
    "    width = 0.8,\n",
    "    alpha = 0.95,\n",
    "    color=colors,\n",
    ")\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylabel(\"Attention Ratio\", fontsize=10)\n",
    "ax.tick_params(axis=\"both\", labelsize=9)\n",
    "plt.errorbar(mean.index, mean, yerr=std, \n",
    "             fmt='.', elinewidth=1.5, markersize=0, capsize=2, color='#59565a')\n",
    "plt.tick_params(bottom = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./ft-ESM_CDR-plot.jpg\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
